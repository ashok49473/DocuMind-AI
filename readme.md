# DocuMind AI ğŸ§ ğŸ“š

### *Transform Your PDFs into Conversational Knowledge*

[![Python](https://img.shields.io/badge/Python-3.9+-blue.svg)](https://python.org)
[![Streamlit](https://img.shields.io/badge/Streamlit-Latest-red.svg)](https://streamlit.io)
[![LangChain](https://img.shields.io/badge/LangChain-Latest-green.svg)](https://langchain.com)
[![OpenAI](https://img.shields.io/badge/OpenAI-GPT--3.5--Turbo-orange.svg)](https://openai.com)
[![Pinecone](https://img.shields.io/badge/Pinecone-v6.0.0-purple.svg)](https://pinecone.io)

---

## ğŸš€ Overview

**DocuMind AI** is an intelligent PDF question-answering system that leverages **Retrieval-Augmented Generation (RAG)** to help users extract insights from PDF documents through natural language conversations. Built with cutting-edge AI technologies, it transforms static documents into an interactive knowledge base with real-time document processing and intelligent responses.

### âœ¨ Key Features

- ğŸ“„ **Dynamic PDF Upload**: Real-time PDF processing with drag-and-drop interface
- ğŸ§  **Semantic Search**: Vector-based similarity search using OpenAI embeddings
- ğŸ¤– **AI-Powered Q&A**: Natural language responses powered by GPT-3.5-Turbo
- ğŸ“Š **Source Attribution**: Transparent citations showing which document sections informed each answer
- ğŸ’¬ **Interactive Interface**: Modern Streamlit web interface with real-time status monitoring
- ğŸ”„ **Modular Architecture**: Clean, maintainable code structure for easy customization
- ğŸ“ˆ **Index Management**: Real-time vector count display and index clearing capabilities

---

## ğŸ› ï¸ Technology Stack

| Component | Technology | Purpose |
|-----------|------------|---------|
| **Framework** | LangChain | AI application orchestration |
| **LLM** | OpenAI GPT-3.5-Turbo | Natural language generation |
| **Vector DB** | Pinecone v6.0.0 | Scalable serverless similarity search |
| **Embeddings** | OpenAI text-embedding-ada-002 | Text vectorization |
| **Frontend** | Streamlit | Interactive web interface |
| **Document Processing** | PyPDF2 | PDF text extraction |

---

## ğŸš€ Quick Start

### Prerequisites

- Python 3.9+
- OpenAI API key
- Pinecone account and API key

### Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/ashok49473/DocuMind-AI.git
   cd DocuMind-AI
   ```

2. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

3. **Set up environment variables**
   ```bash
   # Create .env file
   cp .env.example .env
   
   # Add your API keys to .env
   OPENAI_API_KEY=sk-your-openai-key-here
   PINECONE_API_KEY=your-pinecone-api-key
   PINECONE_INDEX_NAME=documind-ai
   ```

4. **Run the application**
   ```bash
   streamlit run app.py
   ```

5. **Open your browser**
   ```
   Navigate to: http://localhost:8501
   ```

---

## ğŸ“‹ Usage Guide

### Step 1: Upload PDF Document
1. Open the Streamlit interface
2. Use the sidebar file uploader to select a PDF
3. Click **"Process PDF"** to analyze the document
4. Wait for processing confirmation

### Step 2: Ask Questions
1. Enter your question in the main interface text input
2. Click **"Ask Question"**
3. Review the AI-generated response with source citations
4. Expand **"Source Documents"** to see referenced text sections

### Step 3: Manage Your Knowledge Base
- Monitor system status in the right panel
- View real-time vector count statistics
- Clear the vector store to reset the system
- Process new documents to update the knowledge base

---

## ğŸ“ Project Structure

```
DocuMind-AI/
â”œâ”€â”€ ğŸ“„ app.py                    # Main Streamlit application
â”œâ”€â”€ ğŸ“„ config.py                 # Configuration management
â”œâ”€â”€ ğŸ“„ pdf_processor.py          # PDF processing and chunking
â”œâ”€â”€ ğŸ“„ vector_store.py           # Pinecone v6.0.0 integration
â”œâ”€â”€ ğŸ“„ qa_chain.py               # RAG implementation
â”œâ”€â”€ ğŸ“„ requirements.txt          # Python dependencies
â”œâ”€â”€ ğŸ“„ .env.example             # Environment variables template
â””â”€â”€ ğŸ“„ README.md                # This documentation
```

---

## âš™ï¸ Configuration

### Environment Variables

| Variable | Description | Required |
|----------|-------------|----------|
| `OPENAI_API_KEY` | OpenAI API key for GPT-3.5-Turbo and embeddings | âœ… |
| `PINECONE_API_KEY` | Pinecone API key for vector storage | âœ… |
| `PINECONE_INDEX_NAME` | Name for your Pinecone index | âŒ (default: documind-ai) |

### Customizable Parameters

```python
# In config.py
CHUNK_SIZE = 1000              # Document chunk size
CHUNK_OVERLAP = 200            # Overlap between chunks
LLM_MODEL = "gpt-3.5-turbo"    # OpenAI model
EMBEDDING_MODEL = "text-embedding-ada-002"  # Embedding model
PINECONE_DIMENSION = 1536      # Embedding dimension
PINECONE_METRIC = "cosine"     # Similarity metric
PINECONE_CLOUD = "aws"         # Cloud provider
PINECONE_REGION = "us-east-1"  # Region
```

---

## ğŸ”§ Modular Architecture

### Core Components

#### **PDFProcessor**
- Extracts text from uploaded PDF files
- Splits text into manageable chunks with overlap
- Creates LangChain Document objects with metadata

#### **VectorStoreManager**
- Manages Pinecone serverless index operations
- Handles document embedding and storage
- Performs similarity searches with configurable parameters
- Provides index statistics and management

#### **QAChain**
- Implements retrieval-augmented generation
- Uses custom prompts for context-aware responses
- Returns answers with source document attribution
- Handles error cases gracefully

#### **Config**
- Centralized configuration management
- Environment variable validation
- Model and parameter settings
- Pinecone v6.0.0 specifications

---

## ğŸ†• Pinecone v6.0.0 Features

This version leverages the latest Pinecone client with:

- **Serverless Architecture**: Automatic scaling and cost optimization
- **Improved Performance**: Faster indexing and query responses
- **Enhanced API**: Simplified client initialization and management
- **Better Error Handling**: More robust connection and retry logic
- **Real-time Statistics**: Live vector count and index monitoring

---

## ğŸ” How It Works

1. **Document Ingestion**: PDF text is extracted and split into semantic chunks
2. **Embedding Generation**: OpenAI creates vector representations of text chunks
3. **Vector Storage**: Embeddings are stored in Pinecone serverless index
4. **Query Processing**: User questions are embedded and matched against stored vectors
5. **Context Retrieval**: Most relevant document chunks are retrieved
6. **Answer Generation**: GPT-3.5-Turbo generates responses using retrieved context
7. **Source Attribution**: Original document sections are provided for transparency

---

## ğŸ“Š Performance & Limitations

### Strengths
- Handles large PDF documents efficiently
- Provides accurate, contextual responses
- Maintains source attribution for transparency
- Scales automatically with Pinecone serverless

### Considerations
- Requires OpenAI and Pinecone API credits
- Processing time depends on document size
- Accuracy depends on document quality and structure
- Best results with well-structured, text-based PDFs

---

## ğŸ›¡ï¸ Error Handling

The application includes comprehensive error handling for:

- PDF processing failures
- API connection issues
- Vector store operations
- Embedding generation errors
- Question processing failures
- Index management operations

---

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

### Development Guidelines
- Follow the modular architecture pattern
- Add comprehensive error handling
- Include docstrings for all functions
- Test with various PDF formats
- Maintain code readability and documentation

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- **LangChain** team for the incredible RAG framework
- **OpenAI** for powerful language models and embeddings
- **Pinecone** for scalable serverless vector search
- **Streamlit** for the intuitive web framework
- **PyPDF2** for reliable PDF processing

---

## ğŸ“ Contact

**Ashok Kumar** - [ashokpalivela123@gmail.com](mailto:ashokpalivela123@gmail.com)

**Project Link**: [https://github.com/ashok49473/DocuMind-AI](https://github.com/ashok49473/DocuMind-AI)

**Portfolio**: [https://ashok49473.github.io](https://ashok49473.github.io)

---

## ğŸš€ Future Enhancements

- [ ] Multi-document conversation support
- [ ] Advanced filtering and search options
- [ ] Document summarization features
- [ ] Integration with cloud storage services
- [ ] Mobile-responsive interface improvements
- [ ] Batch document processing
- [ ] Custom embedding model support

---

<div align="center">

### â­ Star this project if you found it helpful!

**Built with â¤ï¸ using LangChain, OpenAI, and Pinecone**

*Transform your documents into intelligent conversations*

</div>